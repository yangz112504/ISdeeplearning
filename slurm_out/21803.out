/home/zyang/IS/cudaVsCpu.py:122: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.
  pivot = pivot.applymap(lambda x: f"{x:.3f} ms" if pd.notna(x) else "")
✅ CUDA available: NVIDIA A100 80GB PCIe
Using devices: [device(type='cpu'), device(type='cuda')]
elu (cpu): 0.034389s
hardshrink (cpu): 0.024067s
hardsigmoid (cpu): 0.026897s
hardtanh (cpu): 0.043136s
hardswish (cpu): 0.029559s
leaky_relu (cpu): 0.032748s
logsigmoid (cpu): 0.082440s
prelu (cpu): 0.089284s
relu (cpu): 0.024749s
relu6 (cpu): 0.033455s
rrelu (cpu): 0.125358s
selu (cpu): 0.029173s
celu (cpu): 0.035935s
gelu (cpu): 0.133543s
sigmoid (cpu): 0.029896s
silu (cpu): 0.029943s
mish (cpu): 0.116307s
softplus (cpu): 0.042915s
softshrink (cpu): 0.023365s
softsign (cpu): 0.124591s
tanh (cpu): 0.028171s
tanhshrink (cpu): 0.052238s
threshold (cpu): 0.054367s
glu (cpu): 0.050354s
identity (cpu): 0.001071s
zailuApprox (cpu): 0.349226s
zailuNormal (cpu): 0.282699s
squareplus (cpu): 0.193046s
swish (cpu): 0.053808s
elu (cuda): 0.076101s
hardshrink (cuda): 0.067320s
hardsigmoid (cuda): 0.071714s
hardtanh (cuda): 0.097546s
hardswish (cuda): 0.074040s
leaky_relu (cuda): 0.076709s
logsigmoid (cuda): 0.082584s
prelu (cuda): 0.304619s
relu (cuda): 0.074302s
relu6 (cuda): 0.091209s
rrelu (cuda): 0.117472s
selu (cuda): 0.072958s
celu (cuda): 0.078951s
gelu (cuda): 0.065418s
sigmoid (cuda): 0.064850s
silu (cuda): 0.066743s
mish (cuda): 0.090080s
softplus (cuda): 0.067694s
softshrink (cuda): 0.066947s
softsign (cuda): 0.242454s
tanh (cuda): 0.065639s
tanhshrink (cuda): 0.132399s
threshold (cuda): 0.108755s
glu (cuda): 0.094172s
identity (cuda): 0.001095s
zailuApprox (cuda): 0.637007s
zailuNormal (cuda): 0.391155s
squareplus (cuda): 0.370982s
swish (cuda): 0.134621s

✅ Saved raw benchmark data to activation_benchmarks_raw.csv

=== Formatted Results (ms per 10k runs) ===
device              CPU  GPU (CUDA)
activation                         
identity       1.071 ms    1.095 ms
softshrink    23.365 ms   66.947 ms
hardshrink    24.067 ms   67.320 ms
relu          24.749 ms   74.302 ms
hardsigmoid   26.897 ms   71.714 ms
tanh          28.171 ms   65.639 ms
selu          29.173 ms   72.958 ms
hardswish     29.559 ms   74.040 ms
sigmoid       29.896 ms   64.850 ms
silu          29.943 ms   66.743 ms
leaky_relu    32.748 ms   76.709 ms
relu6         33.455 ms   91.209 ms
elu           34.389 ms   76.101 ms
celu          35.935 ms   78.951 ms
softplus      42.915 ms   67.694 ms
hardtanh      43.136 ms   97.546 ms
glu           50.354 ms   94.172 ms
tanhshrink    52.238 ms  132.399 ms
swish         53.808 ms  134.621 ms
threshold     54.367 ms  108.755 ms
logsigmoid    82.440 ms   82.584 ms
prelu         89.284 ms  304.619 ms
mish         116.307 ms   90.080 ms
softsign     124.591 ms  242.454 ms
rrelu        125.358 ms  117.472 ms
gelu         133.543 ms   65.418 ms
squareplus   193.046 ms  370.982 ms
zailuNormal  282.699 ms  391.155 ms
zailuApprox  349.226 ms  637.007 ms

✅ Saved formatted table to activation_benchmarks_formatted.csv
