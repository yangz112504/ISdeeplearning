/home/zyang/IS/cudaVsCpu.py:122: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.
  pivot = pivot.applymap(lambda x: f"{x:.3f} ms" if pd.notna(x) else "")
✅ CUDA available: NVIDIA A100 80GB PCIe
Using devices: [device(type='cpu'), device(type='cuda')]
elu (cpu): 0.033949s
hardshrink (cpu): 0.022935s
hardsigmoid (cpu): 0.026342s
hardtanh (cpu): 0.041977s
hardswish (cpu): 0.028767s
leaky_relu (cpu): 0.030566s
logsigmoid (cpu): 0.070959s
prelu (cpu): 0.089305s
relu (cpu): 0.025538s
relu6 (cpu): 0.034123s
rrelu (cpu): 0.122127s
selu (cpu): 0.032500s
celu (cpu): 0.036407s
gelu (cpu): 0.146332s
sigmoid (cpu): 0.029936s
silu (cpu): 0.034070s
mish (cpu): 0.118326s
softplus (cpu): 0.044023s
softshrink (cpu): 0.022972s
softsign (cpu): 0.116382s
tanh (cpu): 0.028619s
tanhshrink (cpu): 0.052361s
threshold (cpu): 0.053011s
glu (cpu): 0.060170s
identity (cpu): 0.001155s
zailuApprox (cpu): 0.349230s
zailuNormal (cpu): 0.276189s
squareplus (cpu): 0.191530s
swish (cpu): 0.054583s
elu (cuda): 0.077244s
hardshrink (cuda): 0.067977s
hardsigmoid (cuda): 0.070934s
hardtanh (cuda): 0.102896s
hardswish (cuda): 0.075562s
leaky_relu (cuda): 0.080164s
logsigmoid (cuda): 0.088508s
prelu (cuda): 0.301972s
relu (cuda): 0.075217s
relu6 (cuda): 0.091574s
rrelu (cuda): 0.116553s
selu (cuda): 0.072581s
celu (cuda): 0.080921s
gelu (cuda): 0.068170s
sigmoid (cuda): 0.065058s
silu (cuda): 0.067601s
mish (cuda): 0.091719s
softplus (cuda): 0.068566s
softshrink (cuda): 0.066758s
softsign (cuda): 0.246085s
tanh (cuda): 0.066058s
tanhshrink (cuda): 0.136398s
threshold (cuda): 0.111770s
glu (cuda): 0.095251s
identity (cuda): 0.001015s
zailuApprox (cuda): 0.663949s
zailuNormal (cuda): 0.401235s
squareplus (cuda): 0.377126s
swish (cuda): 0.134128s

✅ Saved raw benchmark data to activation_benchmarks_raw.csv

=== Formatted Results (ms per 10k runs) ===
device              CPU  GPU (CUDA)
activation                         
identity       1.155 ms    1.015 ms
hardshrink    22.935 ms   67.977 ms
softshrink    22.972 ms   66.758 ms
relu          25.538 ms   75.217 ms
hardsigmoid   26.342 ms   70.934 ms
tanh          28.619 ms   66.058 ms
hardswish     28.767 ms   75.562 ms
sigmoid       29.936 ms   65.058 ms
leaky_relu    30.566 ms   80.164 ms
selu          32.500 ms   72.581 ms
elu           33.949 ms   77.244 ms
silu          34.070 ms   67.601 ms
relu6         34.123 ms   91.574 ms
celu          36.407 ms   80.921 ms
hardtanh      41.977 ms  102.896 ms
softplus      44.023 ms   68.566 ms
tanhshrink    52.361 ms  136.398 ms
threshold     53.011 ms  111.770 ms
swish         54.583 ms  134.128 ms
glu           60.170 ms   95.251 ms
logsigmoid    70.959 ms   88.508 ms
prelu         89.305 ms  301.972 ms
softsign     116.382 ms  246.085 ms
mish         118.326 ms   91.719 ms
rrelu        122.127 ms  116.553 ms
gelu         146.332 ms   68.170 ms
squareplus   191.530 ms  377.126 ms
zailuNormal  276.189 ms  401.235 ms
zailuApprox  349.230 ms  663.949 ms

✅ Saved formatted table to activation_benchmarks_formatted.csv
